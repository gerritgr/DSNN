{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cd38d44-4e55-49e9-b32b-1674afea4cc7",
   "metadata": {},
   "source": [
    "# DSNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea3cfe1-7984-4afb-9a66-e25d97b01ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"DSNN\"\n",
    "\n",
    "import sys, os, glob\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import traceback\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import networkx as nx\n",
    "import torch\n",
    "from torch_geometric.utils import to_networkx\n",
    "import numpy as np\n",
    "import scipy.linalg\n",
    "\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f832a0fb-4d49-4262-9e89-bfeee6d87fe5",
   "metadata": {},
   "source": [
    "### Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bc3167-0234-4abc-81be-2c4328213b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 64\n",
    "LAYER_NUM = 9\n",
    "DROPOUT = 0.0\n",
    "LR = 0.0001\n",
    "USE_LAYERNORM = False\n",
    "NUM_EPOCHS = 501\n",
    "USE_RESIDUAL = True\n",
    "NOISE_VAR = 0.005  #regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ad3a44-f458-412b-a980-15a964f4821e",
   "metadata": {},
   "source": [
    "## Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd79e795-843b-4267-b59b-0690bcb68151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minmax_centrality(g):\n",
    "    betweenness_centrality = nx.betweenness_centrality(g)\n",
    "    eigenvector_centrality = nx.eigenvector_centrality(g, max_iter=1000, tol=1e-04)\n",
    "    try:\n",
    "        laplacian_centrality = nx.laplacian_centrality(g)\n",
    "    except:\n",
    "        laplacian_centrality = nx.betweenness_centrality(g)\n",
    "    prod_centrality = list()\n",
    "    \n",
    "    for n in range(g.number_of_nodes()):\n",
    "        v = (betweenness_centrality[n]+0.00001) * (eigenvector_centrality[n]+0.00001) * (laplacian_centrality[n]+0.00001)\n",
    "        prod_centrality.append(v)\n",
    "\n",
    "    min_value = np.min(prod_centrality)\n",
    "    min_nodes = [v for v in range(len(prod_centrality)) if prod_centrality[v]<min_value+0.00000000001]\n",
    "    \n",
    "    max_value = np.max(prod_centrality)\n",
    "    max_nodes = [v for v in range(len(prod_centrality))  if prod_centrality[v]>max_value-0.00000000001 ]\n",
    "\n",
    "    \n",
    "    minmax_centrality = dict()\n",
    "    for n in range(g.number_of_nodes()):\n",
    "        dmin = np.min([nx.shortest_path_length(g, source=n, target=target) for target in min_nodes]) \n",
    "        dmax = np.min([nx.shortest_path_length(g, source=n, target=target) for target in max_nodes]) \n",
    "        minmax_centrality[n] = [dmin, dmax]\n",
    "    return minmax_centrality\n",
    "         \n",
    "    \n",
    "    \n",
    "def convert_graph(g):\n",
    "  g_nx = to_networkx(g, to_undirected=True, node_attrs=['x'])\n",
    "  assert nx.is_connected(g_nx)\n",
    "\n",
    "  minmax_centrality = get_minmax_centrality(g_nx)\n",
    "  for n in list(g_nx.nodes()):\n",
    "    node_data = g_nx.nodes[n] \n",
    "    x_value = node_data.get('x', 1.0) \n",
    "    if 'list' not in str(type(x_value)): \n",
    "      g_nx.nodes[n]['x'] = [x_value] \n",
    "\n",
    "\n",
    "  first_node = list(g_nx.nodes())[0]\n",
    "  orig_size = len(g_nx.nodes[first_node]['x'])  \n",
    "\n",
    "\n",
    "  for node_i, minmax_value in minmax_centrality.items():\n",
    "    assert(\"list\" in str(type(g_nx.nodes[node_i]['x'])))\n",
    "    assert(\"list\" in str(type(minmax_value)))\n",
    "    g_nx.nodes[node_i]['x'] = g_nx.nodes[node_i]['x'] + minmax_value\n",
    "\n",
    "    \n",
    "  betweenness_centrality = nx.betweenness_centrality(g_nx)\n",
    "  for node_i, between_value in betweenness_centrality.items():\n",
    "    g_nx.nodes[node_i]['x'] = g_nx.nodes[node_i]['x'] + [between_value]\n",
    "\n",
    "  degree_centrality = nx.degree_centrality(g_nx)\n",
    "  for node_i, degree_value in degree_centrality.items():\n",
    "    g_nx.nodes[node_i]['x'] = g_nx.nodes[node_i]['x'] + [degree_value]\n",
    "\n",
    "  closeness_centrality = nx.closeness_centrality(g_nx)\n",
    "  for node_i, closeness_value in closeness_centrality.items():\n",
    "    g_nx.nodes[node_i]['x'] = g_nx.nodes[node_i]['x'] + [closeness_value]\n",
    "\n",
    "  second_order_centrality = nx.second_order_centrality(g_nx)\n",
    "  for node_i, second_order_value in closeness_centrality.items():\n",
    "    g_nx.nodes[node_i]['x'] = g_nx.nodes[node_i]['x'] + [second_order_value]\n",
    "\n",
    "  try:\n",
    "    laplacian_centrality = nx.laplacian_centrality(g_nx)\n",
    "    for node_i, laplacian_value in laplacian_centrality.items():\n",
    "      g_nx.nodes[node_i]['x'] = g_nx.nodes[node_i]['x'] + [laplacian_value]\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "  eigenvector_centrality = nx.eigenvector_centrality(g_nx, max_iter=1000, tol=1e-04)\n",
    "  for node_i, eigen_value in eigenvector_centrality.items():\n",
    "    g_nx.nodes[node_i]['x'] = g_nx.nodes[node_i]['x'] + [eigen_value]\n",
    "\n",
    "\n",
    "\n",
    "  x_tensor = torch.zeros([g_nx.number_of_nodes(), len(g_nx.nodes[0]['x'])])\n",
    "  for node_i in g_nx.nodes():\n",
    "    x_i = g_nx.nodes[node_i]['x']\n",
    "    x_tensor[node_i,:] = torch.tensor(x_i)\n",
    " \n",
    "  # Sum over all neighboring nodes\n",
    "  x_tensor2 = torch.zeros_like(x_tensor)\n",
    "  for i in range(x_tensor.shape[0]):\n",
    "    neig_indicator = [i in g_nx.neighbors(i) for i in range(x_tensor.shape[0])]\n",
    "    x_tensor2[node_i,:] = torch.sum(x_tensor[neig_indicator,:], dim=0)\n",
    "\n",
    "  x_tensor = torch.cat((x_tensor,x_tensor2), dim=1)\n",
    "\n",
    "\n",
    "\n",
    "  return g_nx, x_tensor, g.y.clone().detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458302a7-6cd0-4e1f-8501-f06f81c0a2f6",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939abeac-833c-49cc-97e0-1390d863492d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn.conv import x_conv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Sequential\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, layer_num=4, use_residual=USE_RESIDUAL, use_layernorm=USE_LAYERNORM):\n",
    "        super(MLP, self).__init__()\n",
    "        self.mlp_list = nn.ModuleList()\n",
    "        self.use_layernorm = use_layernorm\n",
    "        self.use_residual = use_residual\n",
    "\n",
    "        for i in range(layer_num):\n",
    "            in_dim = hidden_size if i > 0 else input_size\n",
    "            out_dim = hidden_size if i < layer_num-1 else output_size\n",
    "            if use_residual and i>0:\n",
    "              in_dim += input_size\n",
    "            lin = nn.Linear(in_dim, out_dim)\n",
    "            self.mlp_list.append(lin)\n",
    "\n",
    "            if i < layer_num-1:\n",
    "                if use_layernorm and i < layer_num-2:\n",
    "                    self.mlp_list.append(nn.LayerNorm(out_dim))\n",
    "                self.mlp_list.append(nn.ReLU())\n",
    "\n",
    "    def forward(self, x_original):\n",
    "        x = x_original.clone()\n",
    "        for idx, layer in enumerate(self.mlp_list):\n",
    "            if idx>0 and self.use_residual and isinstance(layer, nn.Linear):\n",
    "                x = torch.cat((x, x_original), dim=-1)\n",
    "                x = x.to(DEVICE)\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DeepSet(nn.Module):\n",
    "  def __init__(self, input_size=11, hidden_size=32, output_size=1, use_softmax = False, use_sigmoid=False, layer_num=4, dropout=0.2):\n",
    "    super(DeepSet, self).__init__()\n",
    "    self.mlp1 = MLP(input_size, hidden_size, hidden_size, layer_num=layer_num)\n",
    "    self.mlp2 = MLP(hidden_size, hidden_size, output_size, layer_num=layer_num)\n",
    "    self.use_softmax = use_softmax\n",
    "    self.use_sigmoid = use_sigmoid\n",
    "    self.dropout = dropout\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.mlp1(x)\n",
    "    if self.training and self.dropout > 0.0:\n",
    "      n = x.shape[0]\n",
    "      random_tensor = torch.bernoulli(torch.full((n,), 1.0-self.dropout), device=DEVICE).to(dtype=torch.bool)\n",
    "      x = x[random_tensor, :]\n",
    "\n",
    "    x_sum = torch.sum(x, dim=0)\n",
    "    if NOISE_VAR > 0.0000001 and  self.training: \n",
    "        x_sum += torch.randn_like(x_sum, device=DEVICE)*NOISE_VAR\n",
    "    x = x_sum.flatten()\n",
    "\n",
    "    x = self.mlp2(x)\n",
    "    if self.use_softmax:\n",
    "      return F.softmax(x.flatten(), dim=0)\n",
    "    elif self.use_sigmoid:\n",
    "      return torch.sigmoid(x.flatten())\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5920e815-b6ec-4fda-b1b3-704adcf2590c",
   "metadata": {},
   "source": [
    "## Gen Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c76c74-2cb1-46cc-9049-45d71b529cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(name):\n",
    "  from torch_geometric.datasets import TUDataset\n",
    "  assert name in ['MUTAG', 'PROTEINS', 'ENZYMES', 'IMDB-BINARY']\n",
    "\n",
    "  filename = f'{name}_deepset.pkl'\n",
    "  if os.path.exists(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "      dataset_converted_train, dataset_converted_test, dataset_converted_val = pickle.load(f)\n",
    "    return dataset_converted_train, dataset_converted_test, dataset_converted_val\n",
    "\n",
    "  if name == 'MUTAG':\n",
    "    dataset = TUDataset(root='data/TUDataset', name='MUTAG')\n",
    "  elif name == 'PROTEINS':\n",
    "    dataset = TUDataset(root='data/TUDataset', name='PROTEINS')\n",
    "  elif name == 'ENZYMES':\n",
    "    dataset = TUDataset(root='data/TUDataset', name='ENZYMES')\n",
    "  elif name == 'IMDB-BINARY':\n",
    "    dataset = TUDataset(root='data/TUDataset', name='IMDB-BINARY')\n",
    "    modified_data_list = list()\n",
    "    for data in dataset:\n",
    "        data.x = torch.zeros([data.num_nodes, 1])\n",
    "        modified_data_list.append(data)\n",
    "    dataset = modified_data_list\n",
    "\n",
    "  dataset_converted = list()\n",
    "  for g in tqdm(dataset):\n",
    "    try:\n",
    "      g_nx, x_tensor, y = convert_graph(g)\n",
    "      dataset_converted.append((x_tensor,y))\n",
    "    except AssertionError:\n",
    "      pass\n",
    "\n",
    "  print(\"len dataset_converted\", len(dataset_converted))\n",
    "\n",
    "  random.Random(1234).shuffle(dataset_converted)\n",
    "  split = int(0.8*len(dataset_converted))\n",
    "  dataset_converted_train = dataset_converted[:split]\n",
    "  dataset_converted_testval = dataset_converted[split:]\n",
    "    \n",
    "  split = int(0.5*len(dataset_converted_testval))\n",
    "  dataset_converted_test = dataset_converted_testval[:split]\n",
    "  dataset_converted_val = dataset_converted_testval[split:]\n",
    "\n",
    "\n",
    "  with open(filename, 'wb') as f:\n",
    "    pickle.dump((dataset_converted_train, dataset_converted_test, dataset_converted_val), f)\n",
    "\n",
    "  return dataset_converted_train, dataset_converted_test, dataset_converted_val\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7fec937-2638-4f30-ba41-29fceed1e9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0032bab7-e761-403b-825b-52ce3086c363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, optimizer, model, use_softmax=False):\n",
    "  model.train()\n",
    "  loss_list = list()\n",
    "  for x, y in (dataset):  # Iterate in batches over the training dataset.\n",
    "    x = torch.nan_to_num(x, nan=0.0)\n",
    "    x = x.to(DEVICE)\n",
    "    y = y.to(DEVICE)\n",
    "    out = model(x)  # Perform a single forward pass.\n",
    "    if use_softmax:\n",
    "      loss = (1.0 - out[y.item()])**2\n",
    "    else:\n",
    "      loss = torch.abs(out-y)**2\n",
    "    loss_list.append(loss.item())\n",
    "    loss.backward()  # Derive gradients.\n",
    "    optimizer.step()  # Update parameters based on gradients.\n",
    "    optimizer.zero_grad()  # Clear gradients.\n",
    "  return np.nanmean(loss_list)\n",
    "\n",
    "def test(dataset, model, use_softmax = False):\n",
    "  model.eval()\n",
    "  correct = 0\n",
    "  error_list = list()\n",
    "  print_output = True\n",
    "  for x, y in (dataset):  # Iterate in batches over the training/test dataset.\n",
    "    x = torch.nan_to_num(x, nan=0.0)\n",
    "    x = x.to(DEVICE)\n",
    "    y = y.to(DEVICE)\n",
    "    out = model(x)\n",
    "    if use_softmax:\n",
    "      loss = (1.0 - out[y.item()])\n",
    "      if torch.argmax(out) == y.item():\n",
    "        correct +=1\n",
    "    else:\n",
    "      loss = torch.abs(out-y).item()\n",
    "      assert(y <= 1.0 and y>= 0.0)\n",
    "      correct +=  1 if loss < 0.5 else 0\n",
    "      print_output = False\n",
    "    error_list.append(loss)\n",
    "\n",
    "  return correct/len(dataset) # Derive ratio of correct predictions.\n",
    "\n",
    "\n",
    "def start_agent(name=\"MUTAG\", use_softmax=True):\n",
    "  trainset, testset, valset = get_dataset(name)\n",
    "  atom_dim = trainset[0][0].shape[1]\n",
    "  output_num = 1 #trainset[0][1].numel()\n",
    "  if name == \"ENZYMES\":\n",
    "    output_num = 6\n",
    "\n",
    "  use_softmax = output_num > 1\n",
    "  use_sigmoid = not use_softmax\n",
    "  print(\"Train\", name, 'with input dim', atom_dim, \"and output dim\", output_num)\n",
    "\n",
    "  model = DeepSet(input_size = atom_dim, hidden_size = HIDDEN_DIM, output_size=output_num, use_softmax=use_softmax, use_sigmoid=use_sigmoid, layer_num=LAYER_NUM, dropout=DROPOUT)\n",
    "  model = model.to(DEVICE)\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "  criterion = torch.nn.CrossEntropyLoss()\n",
    "  optimizer.zero_grad()\n",
    "  best_val_acc = -1.0\n",
    "  best_test_acc = -1.0\n",
    "\n",
    "\n",
    "  for epoch in range(1, NUM_EPOCHS):\n",
    "      loss = train(trainset, optimizer, model, use_softmax=use_softmax)\n",
    "      # The final ACC is given as the test ACC at the point where the val ACC is highest. \n",
    "      mean_correct_train = test(trainset, model, use_softmax=use_softmax)\n",
    "      mean_correct_test = test(testset, model, use_softmax=use_softmax)\n",
    "      mean_correct_val = test(valset, model, use_softmax=use_softmax)\n",
    "      if mean_correct_val > best_val_acc:\n",
    "        best_val_acc = mean_correct_val\n",
    "        best_test_acc = mean_correct_test\n",
    "      if epoch % 10 == 0:\n",
    "          print(f'({name}) Epoch: {epoch:03d}, Train Loss: {loss:.4f}, Train Acc: {mean_correct_train:.4f}, Test Acc: {mean_correct_test:.4f}, Val Acc: {mean_correct_val:.4f}, Test Best Acc: {best_test_acc:.4f}')\n",
    "      try:\n",
    "        wandb.log({f\"{name}/test_acc\": mean_correct_test, f\"{name}/train_acc\": mean_correct_train, f\"{name}/val_acc\": mean_correct_val, f\"{name}/besttest_acc\": best_test_acc, f\"{name}/test_loss\": loss})\n",
    "      except:\n",
    "        pass\n",
    "  return mean_correct_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caee1b26-a653-4412-9ffd-fbc7bd6eb771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_experiments():\n",
    "    try:\n",
    "        start_agent(name=\"ENZYMES\", use_softmax=True) \n",
    "    except Exception as e:\n",
    "        error_message = traceback.format_exc()\n",
    "        print(\"final error:\\n\", error_message)\n",
    "        with open('_error_log.txt', 'a') as f:\n",
    "            f.write(error_message + '\\n')\n",
    "\n",
    "    try:\n",
    "        start_agent(name=\"IMDB-BINARY\", use_softmax=False) #\n",
    "    except Exception as e:\n",
    "        error_message = traceback.format_exc()\n",
    "        print(\"final error:\\n\", error_message)\n",
    "        with open('_error_log.txt', 'a') as f:\n",
    "            f.write(error_message + '\\n')\n",
    "\n",
    "    try:\n",
    "        start_agent(name=\"MUTAG\", use_softmax=False) #\n",
    "    except Exception as e:\n",
    "        error_message = traceback.format_exc()\n",
    "        print(\"final error:\\n\", error_message)\n",
    "        with open('_error_log.txt', 'a') as f:\n",
    "            f.write(error_message + '\\n')\n",
    "\n",
    "    try:\n",
    "        start_agent(name=\"PROTEINS\", use_softmax=False) #\n",
    "    except Exception as e:\n",
    "        error_message = traceback.format_exc()\n",
    "        print(\"final error:\\n\", error_message)\n",
    "        with open('_error_log.txt', 'a') as f:\n",
    "            f.write(error_message + '\\n')\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6773987-b516-49bf-a86c-79bfffe78149",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    start_experiments()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb0fc4d-700a-468f-bbaa-b22d35cdddad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
